{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ZpiDdLSI2v"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ME79KRZSLdL"
      },
      "source": [
        "\n",
        "*   The fourth project is the development of a notebook (code + explanation) that successfully engineers 12 unique types of features, **three** for each type of feature engineering: **transforming**, **interacting**, **mapping**, and **extracting**.\n",
        "* The second part of the assignment is the development of a **deep learning classification** model to predict the direction of the S&P500 for the dates **2018-01-01—2018-07-12** (test set).\n",
        "* The feature engineering section is unrelated to the model section, you can develop any features, not just features that would work for deep learning models (later on you can decide which features to use in your model).\n",
        "*  You also have to uncomment all the example features and make them run successfully  → **every** feature example has some error/s that you have to fix. Please also describe the error you fixed!\n",
        "*   Note that we *won't* be attempting to measure the quality of every feature (i.e., how much it improves the model), that is slightly too advanced for this course.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KK3ah2-KR7Mc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8DfulTBSAXT"
      },
      "source": [
        "Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "F8Rf5rSWR-O4",
        "outputId": "e516acc9-8461-4c83-f12f-d7b298896dac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTSE</th>\n",
              "      <th>EuroStoxx50</th>\n",
              "      <th>SP500</th>\n",
              "      <th>Gold</th>\n",
              "      <th>French-2Y</th>\n",
              "      <th>French-5Y</th>\n",
              "      <th>French-10Y</th>\n",
              "      <th>French-30Y</th>\n",
              "      <th>US-2Y</th>\n",
              "      <th>US-5Y</th>\n",
              "      <th>US-10Y</th>\n",
              "      <th>US-30Y</th>\n",
              "      <th>Russel2000</th>\n",
              "      <th>EuroStox_Small</th>\n",
              "      <th>FTSE_Small</th>\n",
              "      <th>MSCI_EM</th>\n",
              "      <th>CRB</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dates</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1989-02-01</th>\n",
              "      <td>2039.7</td>\n",
              "      <td>875.47</td>\n",
              "      <td>297.09</td>\n",
              "      <td>392.50</td>\n",
              "      <td>99.081</td>\n",
              "      <td>99.039</td>\n",
              "      <td>99.572</td>\n",
              "      <td>100.000</td>\n",
              "      <td>100.031</td>\n",
              "      <td>100.345</td>\n",
              "      <td>101.080</td>\n",
              "      <td>101.936</td>\n",
              "      <td>154.38</td>\n",
              "      <td>117.50</td>\n",
              "      <td>1636.57</td>\n",
              "      <td>133.584</td>\n",
              "      <td>286.67</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-02</th>\n",
              "      <td>2043.4</td>\n",
              "      <td>878.08</td>\n",
              "      <td>296.84</td>\n",
              "      <td>392.00</td>\n",
              "      <td>98.898</td>\n",
              "      <td>99.117</td>\n",
              "      <td>99.278</td>\n",
              "      <td>99.692</td>\n",
              "      <td>100.000</td>\n",
              "      <td>100.314</td>\n",
              "      <td>101.017</td>\n",
              "      <td>101.905</td>\n",
              "      <td>154.94</td>\n",
              "      <td>117.69</td>\n",
              "      <td>1642.94</td>\n",
              "      <td>135.052</td>\n",
              "      <td>287.03</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-03</th>\n",
              "      <td>2069.9</td>\n",
              "      <td>884.09</td>\n",
              "      <td>296.97</td>\n",
              "      <td>388.75</td>\n",
              "      <td>98.907</td>\n",
              "      <td>99.002</td>\n",
              "      <td>99.145</td>\n",
              "      <td>99.178</td>\n",
              "      <td>99.812</td>\n",
              "      <td>100.062</td>\n",
              "      <td>100.921</td>\n",
              "      <td>101.718</td>\n",
              "      <td>155.69</td>\n",
              "      <td>118.62</td>\n",
              "      <td>1659.11</td>\n",
              "      <td>137.134</td>\n",
              "      <td>285.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-06</th>\n",
              "      <td>2044.3</td>\n",
              "      <td>885.49</td>\n",
              "      <td>296.04</td>\n",
              "      <td>388.00</td>\n",
              "      <td>98.484</td>\n",
              "      <td>98.502</td>\n",
              "      <td>98.510</td>\n",
              "      <td>97.739</td>\n",
              "      <td>99.812</td>\n",
              "      <td>100.062</td>\n",
              "      <td>100.794</td>\n",
              "      <td>101.468</td>\n",
              "      <td>155.58</td>\n",
              "      <td>118.89</td>\n",
              "      <td>1656.86</td>\n",
              "      <td>137.037</td>\n",
              "      <td>284.69</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-07</th>\n",
              "      <td>2072.8</td>\n",
              "      <td>883.82</td>\n",
              "      <td>299.63</td>\n",
              "      <td>392.75</td>\n",
              "      <td>98.438</td>\n",
              "      <td>98.312</td>\n",
              "      <td>98.292</td>\n",
              "      <td>97.688</td>\n",
              "      <td>99.906</td>\n",
              "      <td>100.251</td>\n",
              "      <td>101.144</td>\n",
              "      <td>102.092</td>\n",
              "      <td>156.84</td>\n",
              "      <td>118.28</td>\n",
              "      <td>1662.76</td>\n",
              "      <td>136.914</td>\n",
              "      <td>284.21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              FTSE  EuroStoxx50   SP500    Gold  French-2Y  French-5Y  \\\n",
              "Dates                                                                   \n",
              "1989-02-01  2039.7       875.47  297.09  392.50     99.081     99.039   \n",
              "1989-02-02  2043.4       878.08  296.84  392.00     98.898     99.117   \n",
              "1989-02-03  2069.9       884.09  296.97  388.75     98.907     99.002   \n",
              "1989-02-06  2044.3       885.49  296.04  388.00     98.484     98.502   \n",
              "1989-02-07  2072.8       883.82  299.63  392.75     98.438     98.312   \n",
              "\n",
              "            French-10Y  French-30Y    US-2Y    US-5Y   US-10Y   US-30Y  \\\n",
              "Dates                                                                    \n",
              "1989-02-01      99.572     100.000  100.031  100.345  101.080  101.936   \n",
              "1989-02-02      99.278      99.692  100.000  100.314  101.017  101.905   \n",
              "1989-02-03      99.145      99.178   99.812  100.062  100.921  101.718   \n",
              "1989-02-06      98.510      97.739   99.812  100.062  100.794  101.468   \n",
              "1989-02-07      98.292      97.688   99.906  100.251  101.144  102.092   \n",
              "\n",
              "            Russel2000  EuroStox_Small  FTSE_Small  MSCI_EM     CRB  target  \n",
              "Dates                                                                        \n",
              "1989-02-01      154.38          117.50     1636.57  133.584  286.67       0  \n",
              "1989-02-02      154.94          117.69     1642.94  135.052  287.03       1  \n",
              "1989-02-03      155.69          118.62     1659.11  137.134  285.63       0  \n",
              "1989-02-06      155.58          118.89     1656.86  137.037  284.69       1  \n",
              "1989-02-07      156.84          118.28     1662.76  136.914  284.21       0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preparing our data\n",
        "raw_prices = pd.read_csv(\"https://storage.googleapis.com/sovai-public/random/assetalloc.csv\", sep=';', parse_dates=True, index_col='Dates', dayfirst=True)\n",
        "df = raw_prices.sort_values(by='Dates')\n",
        "df[\"target\"] = df[\"SP500\"].pct_change().shift(-1)\n",
        "df[\"target\"] = np.where(df[\"target\"]>0,1,0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KjWNbXqfZju"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IZjqGkgbfYIb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df.pop(\"target\")\n",
        "X = df.copy()\n",
        "\n",
        "X_train = X[X.index.astype(str)<'2018-01-01']\n",
        "y_train = y[X_train.index]\n",
        "X_test = X[~X.index.isin(X_train.index)]\n",
        "y_test = y[X_test.index]\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8khPX0DTStdP"
      },
      "source": [
        "### Transforming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ZyMnQ__B49"
      },
      "source": [
        "1. Refresh your mind on tranformation methods by going back to the material. I am simply providing 1 example here.\n",
        "1. Don't repeat my logarithmic return calculation, develop your own transformation (there are 1000s of types of transformations).\n",
        "1. In the example I provide, there is also an error that you have to fix. For example, one of the errors below is that you should actually use `np.log1p()`, but there is another one, so watch out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4dwunC3GSrK2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTSE</th>\n",
              "      <th>EuroStoxx50</th>\n",
              "      <th>SP500</th>\n",
              "      <th>Gold</th>\n",
              "      <th>French-2Y</th>\n",
              "      <th>French-5Y</th>\n",
              "      <th>French-10Y</th>\n",
              "      <th>French-30Y</th>\n",
              "      <th>US-2Y</th>\n",
              "      <th>US-5Y</th>\n",
              "      <th>US-10Y</th>\n",
              "      <th>US-30Y</th>\n",
              "      <th>Russel2000</th>\n",
              "      <th>EuroStox_Small</th>\n",
              "      <th>FTSE_Small</th>\n",
              "      <th>MSCI_EM</th>\n",
              "      <th>CRB</th>\n",
              "      <th>FTSE_log</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dates</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1989-02-01</th>\n",
              "      <td>2039.7</td>\n",
              "      <td>875.47</td>\n",
              "      <td>297.09</td>\n",
              "      <td>392.50</td>\n",
              "      <td>99.081</td>\n",
              "      <td>99.039</td>\n",
              "      <td>99.572</td>\n",
              "      <td>100.000</td>\n",
              "      <td>100.031</td>\n",
              "      <td>100.345</td>\n",
              "      <td>101.080</td>\n",
              "      <td>101.936</td>\n",
              "      <td>154.38</td>\n",
              "      <td>117.50</td>\n",
              "      <td>1636.57</td>\n",
              "      <td>133.584</td>\n",
              "      <td>286.67</td>\n",
              "      <td>7.620558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-02</th>\n",
              "      <td>2043.4</td>\n",
              "      <td>878.08</td>\n",
              "      <td>296.84</td>\n",
              "      <td>392.00</td>\n",
              "      <td>98.898</td>\n",
              "      <td>99.117</td>\n",
              "      <td>99.278</td>\n",
              "      <td>99.692</td>\n",
              "      <td>100.000</td>\n",
              "      <td>100.314</td>\n",
              "      <td>101.017</td>\n",
              "      <td>101.905</td>\n",
              "      <td>154.94</td>\n",
              "      <td>117.69</td>\n",
              "      <td>1642.94</td>\n",
              "      <td>135.052</td>\n",
              "      <td>287.03</td>\n",
              "      <td>7.622370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-03</th>\n",
              "      <td>2069.9</td>\n",
              "      <td>884.09</td>\n",
              "      <td>296.97</td>\n",
              "      <td>388.75</td>\n",
              "      <td>98.907</td>\n",
              "      <td>99.002</td>\n",
              "      <td>99.145</td>\n",
              "      <td>99.178</td>\n",
              "      <td>99.812</td>\n",
              "      <td>100.062</td>\n",
              "      <td>100.921</td>\n",
              "      <td>101.718</td>\n",
              "      <td>155.69</td>\n",
              "      <td>118.62</td>\n",
              "      <td>1659.11</td>\n",
              "      <td>137.134</td>\n",
              "      <td>285.63</td>\n",
              "      <td>7.635256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-06</th>\n",
              "      <td>2044.3</td>\n",
              "      <td>885.49</td>\n",
              "      <td>296.04</td>\n",
              "      <td>388.00</td>\n",
              "      <td>98.484</td>\n",
              "      <td>98.502</td>\n",
              "      <td>98.510</td>\n",
              "      <td>97.739</td>\n",
              "      <td>99.812</td>\n",
              "      <td>100.062</td>\n",
              "      <td>100.794</td>\n",
              "      <td>101.468</td>\n",
              "      <td>155.58</td>\n",
              "      <td>118.89</td>\n",
              "      <td>1656.86</td>\n",
              "      <td>137.037</td>\n",
              "      <td>284.69</td>\n",
              "      <td>7.622811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989-02-07</th>\n",
              "      <td>2072.8</td>\n",
              "      <td>883.82</td>\n",
              "      <td>299.63</td>\n",
              "      <td>392.75</td>\n",
              "      <td>98.438</td>\n",
              "      <td>98.312</td>\n",
              "      <td>98.292</td>\n",
              "      <td>97.688</td>\n",
              "      <td>99.906</td>\n",
              "      <td>100.251</td>\n",
              "      <td>101.144</td>\n",
              "      <td>102.092</td>\n",
              "      <td>156.84</td>\n",
              "      <td>118.28</td>\n",
              "      <td>1662.76</td>\n",
              "      <td>136.914</td>\n",
              "      <td>284.21</td>\n",
              "      <td>7.636656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              FTSE  EuroStoxx50   SP500    Gold  French-2Y  French-5Y  \\\n",
              "Dates                                                                   \n",
              "1989-02-01  2039.7       875.47  297.09  392.50     99.081     99.039   \n",
              "1989-02-02  2043.4       878.08  296.84  392.00     98.898     99.117   \n",
              "1989-02-03  2069.9       884.09  296.97  388.75     98.907     99.002   \n",
              "1989-02-06  2044.3       885.49  296.04  388.00     98.484     98.502   \n",
              "1989-02-07  2072.8       883.82  299.63  392.75     98.438     98.312   \n",
              "\n",
              "            French-10Y  French-30Y    US-2Y    US-5Y   US-10Y   US-30Y  \\\n",
              "Dates                                                                    \n",
              "1989-02-01      99.572     100.000  100.031  100.345  101.080  101.936   \n",
              "1989-02-02      99.278      99.692  100.000  100.314  101.017  101.905   \n",
              "1989-02-03      99.145      99.178   99.812  100.062  100.921  101.718   \n",
              "1989-02-06      98.510      97.739   99.812  100.062  100.794  101.468   \n",
              "1989-02-07      98.292      97.688   99.906  100.251  101.144  102.092   \n",
              "\n",
              "            Russel2000  EuroStox_Small  FTSE_Small  MSCI_EM     CRB  FTSE_log  \n",
              "Dates                                                                          \n",
              "1989-02-01      154.38          117.50     1636.57  133.584  286.67  7.620558  \n",
              "1989-02-02      154.94          117.69     1642.94  135.052  287.03  7.622370  \n",
              "1989-02-03      155.69          118.62     1659.11  137.134  285.63  7.635256  \n",
              "1989-02-06      155.58          118.89     1656.86  137.037  284.69  7.622811  \n",
              "1989-02-07      156.84          118.28     1662.76  136.914  284.21  7.636656  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example Transforming (has errors)\n",
        "\n",
        "# Name: Logarithmic return of FTSE\n",
        "# Description: Developing the logarithmic return feature for use within linear models that make normality assumptions.\n",
        "\n",
        "# df[\"FTSE_log\"] = np.log(df[\"FTSE\"])\n",
        "df[\"FTSE_log\"] = np.loglp(df[\"FTSE\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JvCCtvoT_2a"
      },
      "outputs": [],
      "source": [
        "## Transforming 1 (Add code below)\n",
        "df[\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOCuKHEmT_wU"
      },
      "outputs": [],
      "source": [
        "## Transforming 2 (Add code below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghn81UtvPYmF"
      },
      "outputs": [],
      "source": [
        "## Transforming 3 (Add code below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiyIliYJTJRN"
      },
      "source": [
        "### Interacting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWuduQ-m_q7R"
      },
      "source": [
        "There are millions of possible interaction methods, be creative and come up with your own. For this assignment there is no 'right' feature engineering method, you simply develop one, and give it a name and a discreption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IN3tnhNUTDK3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20488/1229489511.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"gold_r__div__teny_r\"] = gold_returns/teny_returns\n",
            "/tmp/ipykernel_20488/1229489511.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"gold_r__div__teny_r\"] = gold_returns/teny_returns\n"
          ]
        }
      ],
      "source": [
        "# Example Interacting (has errors)\n",
        "\n",
        "# Name: Ratio of Gold return to 10Y treasury\n",
        "# Desciption: Both gold and treasuries are safe-haven assets and descrepency in their ratio could be a sign of some marco-economic event.\n",
        "\n",
        "def gold_to_yield(df):\n",
        "  teny_returns = df[\"US-10Y\"].pct_change()\n",
        "  gold_returns = df[\"Gold\"]\n",
        "  df[\"gold_r__div__teny_r\"] = gold_returns/teny_returns\n",
        "  return df\n",
        "\n",
        "X_train = gold_to_yield(X_train); X_test = gold_to_yield(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwB5gEVjT1dz"
      },
      "outputs": [],
      "source": [
        "## Interacting 1 (Add code below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-TYsjyUTuF2"
      },
      "outputs": [],
      "source": [
        "## Interacting 2 (Add code below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iClfjUDPWu2"
      },
      "outputs": [],
      "source": [
        "## Interacting 3 (Add code below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwVvQ7NKdy6B"
      },
      "source": [
        "### Mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Rc0htlDnrA"
      },
      "source": [
        "This one is slightly harder, you have to identify other  dimensionality reduction methods, there are many more than just PCA. Maybe you can also look at performing the decompositions just on a single asset classes, e.g., US-2Y, US-5Y, US-10Y, US-30Y is a fixed income asset class, but there are a few others in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IuR-aeFNd0LR"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input X contains infinity or a value too large for dtype('float64').",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m   X_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_prinicipal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(X_test_s\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test\n\u001b[0;32m---> 20\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mpca_first\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# # ValueError: Input contains infinity or a value too large for dtype('float64').\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mpca_first\u001b[0;34m(X_train, X_test)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpca_first\u001b[39m(X_train, X_test):\n\u001b[1;32m     11\u001b[0m   sc \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 12\u001b[0m   X_train_s \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m   X_test_s \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtransform(X_test\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     15\u001b[0m   pca \u001b[38;5;241m=\u001b[39m PCA(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 873\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
          ]
        }
      ],
      "source": [
        "# Example Mapping (has errors)\n",
        "\n",
        "# Name: First prinicipal component of all of the assets returns\n",
        "# Description:For stocks the first component resmbles the return of the market, for multiple asset classes it could resemble a 'universal' asset class\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def pca_first(X_train, X_test):\n",
        "  sc = StandardScaler()\n",
        "  X_train_s = sc.fit_transform(X_train.fillna(0))\n",
        "  X_test_s = sc.transform(X_test.fillna(0))\n",
        "\n",
        "  pca = PCA(1)\n",
        "  X_train[\"first_prinicipal\"] = pca.fit_transform(X_train_s.fillna(0))\n",
        "  X_test[\"first_prinicipal\"] = pca.transform(X_test_s.fillna(0))\n",
        "  return X_train, X_test\n",
        "\n",
        "X_train, X_test = pca_first(X_train, X_test)\n",
        "\n",
        "# # ValueError: Input contains infinity or a value too large for dtype('float64')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XAoLXiajaIO"
      },
      "outputs": [],
      "source": [
        "## Mapping 1 (Add code below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUmRlTldjb63"
      },
      "outputs": [],
      "source": [
        "## Mapping 2 (Add code below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh6vU15ePTKm"
      },
      "outputs": [],
      "source": [
        "## Mapping 3 (Add code below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtRBRXeRd_ad"
      },
      "source": [
        "Extracting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lHpV2LxIeA9i"
      },
      "outputs": [],
      "source": [
        "# Example Extracting (has errors)\n",
        "# Name: Annualized volatility in returns\n",
        "# Description: We are developing an annualized volatility measure for all asset returns, which is a good measure of market turbulence\n",
        "\n",
        "def vola(df):\n",
        "  volatility = df.pct_change().rolling(window=365).std()*(365**0.5)\n",
        "  new_names = [(i,i+'_vol') for i in df.columns.values]\n",
        "  volatility.rename(columns = dict(new_names), inplace=True)\n",
        "  df = pd.concat((df, volatility), axis=1)\n",
        "  return df\n",
        "\n",
        "X_train = vola(X_train); X_test = vola(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P165fWF9PNdx"
      },
      "outputs": [],
      "source": [
        "## Extracting 1 (Add code below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GODtIouyPQK-"
      },
      "outputs": [],
      "source": [
        "## Extracting 2 (Add code below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi2MaoAcPRFH"
      },
      "outputs": [],
      "source": [
        "## Extracting 3 (Add code below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhDvqiPnHccB"
      },
      "source": [
        "## Deep Learning Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoNmgbgWHjGs"
      },
      "source": [
        "* For the deep learning model you can perform new data preprocessing methods and new feature engineering that are better suited to neural networks. You can also use all or some of the features you developed above (most features work in deep learning models as long as they are normalized).\n",
        "* It is very hard to predict the stock price, so in my grading I will look more at the quality of the model you process (e.g., that there is no data leakage, that you performed some hyperparameter tuning).\n",
        "* Make sure that you switch your GPU on, you have access to it on Colab. The training stage also takes long, you might want to use a smaller amount of data, or fewer epochs at first to speed up your development process.\n",
        "* After your training is done, you don't have to save your model, but you do have to print the performance of your model. You can report two metrics the ROC(AUC) and the Accuracy against the test set.\n",
        "* Also remember to set the random seed (random state) so that when I run your software, I get similar results (the results doesn't have to be exactely the same).\n",
        "* You can choose any type of deep learning archetecture, e.g., LSTM, GRU, CNN, it is up to you.\n",
        "* Remember that this section is less that 25% of the grade, so don't waste your time here.\n",
        "* And lastly, remember this is the stock market, so it is **difficult** to have an accuracy above 50%, good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzk2wizgHwlP"
      },
      "outputs": [],
      "source": [
        "## Implement Here\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
